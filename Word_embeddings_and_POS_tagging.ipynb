{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_embeddings_and_POS_tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "5JwYnCf79D8G"
      },
      "cell_type": "markdown",
      "source": [
        "# Code setup same as \"final_project\" notebook except limited to Unigrams. Jump to \"Explore Top Words\" section for new code."
      ]
    },
    {
      "metadata": {
        "id": "mDPmRHlAj-Qa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "e58f0dc2-dfdf-475a-ef25-fac802fc4cfb"
      },
      "cell_type": "code",
      "source": [
        "# import nltk library\n",
        "import nltk; nltk.download('punkt')\n",
        "from nltk import sent_tokenize, pos_tag\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# import stopword libraries\n",
        "nltk.download('stopwords'); from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction import stop_words\n",
        "\n",
        "# import other libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string \n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.feature_extraction.text import *\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.grid_search import GridSearchCV\n",
        "from sklearn.metrics import *\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "\n",
        "# import clustering library\n",
        "from nltk.cluster.kmeans import KMeansClusterer\n",
        "from nltk.cluster.util import cosine_distance\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# import word embedding library\n",
        "import glove_helper\n",
        "\n",
        "# import helper libraries\n",
        "import collections\n",
        "from common import utils, vocabulary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /content/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f4035a7690ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# import word embedding library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mglove_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# import helper libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'glove_helper'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "F-U8RIZSj-Qj"
      },
      "cell_type": "markdown",
      "source": [
        "## Load data\n",
        "\n",
        "load sample data of restaurant businesses "
      ]
    },
    {
      "metadata": {
        "id": "eaQYOWyMj-Qk"
      },
      "cell_type": "code",
      "source": [
        "# load data\n",
        "\n",
        "sample_df=pd.read_csv(\"restaurants_only_804868.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bFOqvlUj-Qn"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess data\n",
        "\n",
        "clean text data, recode variables, identify stop words, create train/test sets"
      ]
    },
    {
      "metadata": {
        "id": "0Jzfkqcjj-Qo"
      },
      "cell_type": "markdown",
      "source": [
        "### clean text data"
      ]
    },
    {
      "metadata": {
        "id": "XHZtmGRij-Qo",
        "outputId": "8c872a52-2263-4227-b68e-6823f0d29904"
      },
      "cell_type": "code",
      "source": [
        "example_text=\"\"\"Very pleased with the service. Friendly, attentive, and fast. I had vegetable egg rolls and Pad Thai. \n",
        "              Pad Thai was exquisite. Not too oily or too dry, just perfect. Just the right amount of food on the plate, \n",
        "              the tofu was baked to perfection and made the flavor stand out. The egg rolls were crispy but not over \n",
        "              fried and not oily either. Definitely coming back and recommending it to friends.\"\"\"\n",
        "\n",
        "# tokenize\n",
        "def tokenize_text(input_text):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "    input_text: a string representing an \n",
        "    individual review\n",
        "        \n",
        "    Returns:\n",
        "    input_token: a list containing stemmed \n",
        "    tokens, with punctutations removed, for \n",
        "    an individual review\n",
        "        \n",
        "    \"\"\"\n",
        "    input_tokens=[]\n",
        "    sb=SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "    translator=str.maketrans('', '', string.punctuation)\n",
        "        \n",
        "    # Split sentence\n",
        "    sents=sent_tokenize(input_text)\n",
        "            \n",
        "    # Split word\n",
        "    for sent in sents:\n",
        "        sent=sent.translate(translator)\n",
        "        #input_tokens+=[sb.stem(token) for token in TreebankWordTokenizer().tokenize(sent)] #stem\n",
        "        input_tokens+=TreebankWordTokenizer().tokenize(sent) #don't stem\n",
        "        \n",
        "    return input_tokens\n",
        "\n",
        "\n",
        "# canonicalize\n",
        "def canonicalize_tokens(input_tokens):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    input_tokens: a list containing tokenized \n",
        "    tokens for an individual review\n",
        "    \n",
        "    Returns:\n",
        "    input_tokens: a list containing canonicalized \n",
        "    tokens for an individual review\n",
        "    \n",
        "    \"\"\"\n",
        "    input_tokens=utils.canonicalize_words(input_tokens)\n",
        "    return input_tokens\n",
        "\n",
        "# return nouns only\n",
        "\n",
        "def nouns_only(canonical_tokens):\n",
        "    \n",
        "    tagged_tokens = pos_tag(canonical_tokens)\n",
        "    \n",
        "    return tagged_tokens\n",
        "\n",
        "\n",
        "\n",
        "# preprocessor \n",
        "def preprocessor(raw_text):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    raw_text: a string representing an\n",
        "    individual review\n",
        "    \n",
        "    Returns:\n",
        "    preprocessed_text: a string representing \n",
        "    a preprocessed individual review\n",
        "    \n",
        "    \"\"\"\n",
        "    # tokenize\n",
        "    tokens=tokenize_text(raw_text)\n",
        "    \n",
        "    # canonicalize\n",
        "    canonical_tokens=canonicalize_tokens(tokens)\n",
        "    \n",
        "    # rejoin string\n",
        "    preprocessed_text=(\" \").join(canonical_tokens) \n",
        "    return preprocessed_text\n",
        "\n",
        "# example data\n",
        "# input_tokens=tokenize_text(example_text)\n",
        "# print(\"\\nINPUT TOKENS:\", input_tokens)\n",
        "\n",
        "# canonical_tokens=canonicalize_tokens(input_tokens)\n",
        "# print(\"\\nCANONICAL TOKENS:\", canonical_tokens)\n",
        "\n",
        "# tagged_tokens = nouns_only(canonical_tokens)\n",
        "# print(\"\\nTAGGED TOKENS:\", tagged_tokens)\n",
        "\n",
        "# preprocessed_text=preprocessor(example_text) \n",
        "# print(\"\\nPREPROCESSED TEXT:\", preprocessed_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "INPUT TOKENS: ['Very', 'pleased', 'with', 'the', 'service', 'Friendly', 'attentive', 'and', 'fast', 'I', 'had', 'vegetable', 'egg', 'rolls', 'and', 'Pad', 'Thai', 'Pad', 'Thai', 'was', 'exquisite', 'Not', 'too', 'oily', 'or', 'too', 'dry', 'just', 'perfect', 'Just', 'the', 'right', 'amount', 'of', 'food', 'on', 'the', 'plate', 'the', 'tofu', 'was', 'baked', 'to', 'perfection', 'and', 'made', 'the', 'flavor', 'stand', 'out', 'The', 'egg', 'rolls', 'were', 'crispy', 'but', 'not', 'over', 'fried', 'and', 'not', 'oily', 'either', 'Definitely', 'coming', 'back', 'and', 'recommending', 'it', 'to', 'friends']\n",
            "\n",
            "CANONICAL TOKENS: ['very', 'pleased', 'with', 'the', 'service', 'friendly', 'attentive', 'and', 'fast', 'i', 'had', 'vegetable', 'egg', 'rolls', 'and', 'pad', 'thai', 'pad', 'thai', 'was', 'exquisite', 'not', 'too', 'oily', 'or', 'too', 'dry', 'just', 'perfect', 'just', 'the', 'right', 'amount', 'of', 'food', 'on', 'the', 'plate', 'the', 'tofu', 'was', 'baked', 'to', 'perfection', 'and', 'made', 'the', 'flavor', 'stand', 'out', 'the', 'egg', 'rolls', 'were', 'crispy', 'but', 'not', 'over', 'fried', 'and', 'not', 'oily', 'either', 'definitely', 'coming', 'back', 'and', 'recommending', 'it', 'to', 'friends']\n",
            "\n",
            "TAGGED TOKENS: [('very', 'RB'), ('pleased', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('service', 'NN'), ('friendly', 'RB'), ('attentive', 'JJ'), ('and', 'CC'), ('fast', 'JJ'), ('i', 'NN'), ('had', 'VBD'), ('vegetable', 'JJ'), ('egg', 'NN'), ('rolls', 'NNS'), ('and', 'CC'), ('pad', 'NN'), ('thai', 'NN'), ('pad', 'NN'), ('thai', 'NN'), ('was', 'VBD'), ('exquisite', 'JJ'), ('not', 'RB'), ('too', 'RB'), ('oily', 'RB'), ('or', 'CC'), ('too', 'RB'), ('dry', 'JJ'), ('just', 'RB'), ('perfect', 'VB'), ('just', 'RB'), ('the', 'DT'), ('right', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('food', 'NN'), ('on', 'IN'), ('the', 'DT'), ('plate', 'NN'), ('the', 'DT'), ('tofu', 'NN'), ('was', 'VBD'), ('baked', 'VBN'), ('to', 'TO'), ('perfection', 'NN'), ('and', 'CC'), ('made', 'VBD'), ('the', 'DT'), ('flavor', 'NN'), ('stand', 'VBP'), ('out', 'RP'), ('the', 'DT'), ('egg', 'NN'), ('rolls', 'NNS'), ('were', 'VBD'), ('crispy', 'NNS'), ('but', 'CC'), ('not', 'RB'), ('over', 'IN'), ('fried', 'VBN'), ('and', 'CC'), ('not', 'RB'), ('oily', 'RB'), ('either', 'CC'), ('definitely', 'RB'), ('coming', 'VBG'), ('back', 'RB'), ('and', 'CC'), ('recommending', 'VBG'), ('it', 'PRP'), ('to', 'TO'), ('friends', 'VB')]\n",
            "\n",
            "PREPROCESSED TEXT: very pleased with the service friendly attentive and fast i had vegetable egg rolls and pad thai pad thai was exquisite not too oily or too dry just perfect just the right amount of food on the plate the tofu was baked to perfection and made the flavor stand out the egg rolls were crispy but not over fried and not oily either definitely coming back and recommending it to friends\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TzMDjUIwj-Qt"
      },
      "cell_type": "markdown",
      "source": [
        "### recode variables"
      ]
    },
    {
      "metadata": {
        "id": "pB7C2x6wj-Qt",
        "outputId": "b1b78ba4-9f46-46dd-9e39-6d692ff415af"
      },
      "cell_type": "code",
      "source": [
        "# get reviews, ratings data\n",
        "text=sample_df[\"text\"].tolist() # list of strings\n",
        "labels=sample_df[\"stars\"].tolist() # list of integers\n",
        "\n",
        "# recode labels: ratings 1,2 vs. 3 vs. 4,5\n",
        "def trinary_recode(x):\n",
        "    if x==1 or x==2: x=0 # includes ratings 1,2\n",
        "    elif x==4 or x==5: x=2 # includes ratings 4,5\n",
        "    else: x=1 # includes only rating 3    \n",
        "    return x\n",
        "\n",
        "recoded_trinary_labels=list(map(trinary_recode, labels))\n",
        "\n",
        "# recode labels: ratings 1,2 vs. 4,5 (exclude rating 3)\n",
        "#filtered_df=sample_df.drop(sample_df[sample_df.stars==3].index)\n",
        "#filtered_text=filtered_df[\"text\"].tolist() \n",
        "#filtered_labels=filtered_df[\"stars\"].tolist() \n",
        "\n",
        "#def binary_recode(x):\n",
        "    #if x==1 or x==2: x=0 # includes ratings 1,2\n",
        "    #else: x=1 # includes ratings 4,5    \n",
        "    #return x\n",
        "\n",
        "#recoded_binary_labels=list(map(binary_recode, filtered_labels))\n",
        "\n",
        "print(text[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I usually LOVE hot n juicy, but today's food was just eh. I got a pound of shrimp, with garlic butter sauce, mild heat. I like my shrimps to be fresh; not all mushy and gross. Half of my shrimp was all mushy and I definitely lost my appetite. When my shrimp came in a bag, it was very securely tied at the top with a layer of two bags; that tells me that it's been in that sauce for way too long. I was super disappointed. The waiters we had were super attentive, although one of them seemed to be in a hurry because she kept grabbing the check when we haven't paid yet and asking if the check was ready when clearly we were still eating. Very unhappy with the hot n juicy on eastern, maybe the one on spring mountain is better. I used to like hot n juicy better than boiling crab but now I'm not so sure.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ecnRV_vZj-Qw"
      },
      "cell_type": "markdown",
      "source": [
        "### identify stopwords"
      ]
    },
    {
      "metadata": {
        "id": "bZ7h4QFej-Qx",
        "outputId": "01b0b9a2-9dfc-4faf-875d-83041929eecd"
      },
      "cell_type": "code",
      "source": [
        "# sklearn stopwords (frozenset)\n",
        "sklearn_stopwords=stop_words.ENGLISH_STOP_WORDS\n",
        "print(\"number of sklearn stopwords: %d\" %(len(sklearn_stopwords)))\n",
        "#print(sklearn_stopwords)\n",
        "\n",
        "# nltk stopwords (list)\n",
        "nltk_stopwords=stopwords.words(\"english\")\n",
        "print(\"number of nltk stopwords: %d\" %(len(nltk_stopwords)))\n",
        "#print(nltk_stopwords)\n",
        "\n",
        "# combined sklearn, nltk, other stopwords (set)\n",
        "total_stopwords=set(list(sklearn_stopwords.difference(set(nltk_stopwords)))+nltk_stopwords)\n",
        "\n",
        "other_stopwords=[\"DG\", \"DGDG\"]\n",
        "for w in other_stopwords:\n",
        "    total_stopwords.add(w)\n",
        "    \n",
        "print(\"number of total stopwords: %d\" %(len(total_stopwords)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of sklearn stopwords: 318\n",
            "number of nltk stopwords: 179\n",
            "number of total stopwords: 380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3FcHIiXsj-Qz"
      },
      "cell_type": "markdown",
      "source": [
        "### create train, test sets"
      ]
    },
    {
      "metadata": {
        "id": "-CEa3v--j-Q1",
        "outputId": "d3e0e61c-03d6-4088-adbc-76d50f3a1289"
      },
      "cell_type": "code",
      "source": [
        "# using all labels\n",
        "#train_data, test_data, train_labels, test_labels=train_test_split(text, labels, test_size=.2) \n",
        "\n",
        "# using recoded labels\n",
        "train_data, test_data, train_labels, test_labels=train_test_split(text, recoded_trinary_labels, test_size=.2,\n",
        "                                                                   random_state=101)\n",
        "\n",
        "# examine train, test shapes\n",
        "print(\"train, test set size: %d, %d\" %(len(train_data), len(test_data))) \n",
        "print(\"train, test label size: %d, %d\" %(len(train_labels), len(test_labels)))\n",
        "print(\"\")\n",
        "\n",
        "# examine train set examples\n",
        "print(\"example:\")\n",
        "print(\"text: %s\" %(train_data[1]))\n",
        "print(\"label: %d\" %(train_labels[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train, test set size: 643894, 160974\n",
            "train, test label size: 643894, 160974\n",
            "\n",
            "example:\n",
            "text: Took the girl out for a date night last October.  It's not a cheap date, but it was a very nice atmosphere for a date.  The nights were still reasonably warm, so sitting on the patio was lovely.\n",
            "\n",
            "The salmon appetizer is to die for.  Yes, I'm using that phrase, to die for.  The seared ahi tuna was like butter . . . amazing.\n",
            "\n",
            "I went back again in February for happy hour, sat at the outdoor bar.  Bartender was impeccable, drinks were great, service fantastic.\n",
            "label: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wmpt-SjJj-Q8"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline model: Error Analysis\n",
        "\n",
        "based on baseline model, the model shows decent performance but has greatest difficulty classifying rating category 3. model may be improved by examining class imbalance issues, addressing CountVectorizer limitations, etc."
      ]
    },
    {
      "metadata": {
        "id": "Sp75REI9j-Q-",
        "outputId": "bdd09902-b0c7-4520-c0bb-6d9c7625c274"
      },
      "cell_type": "code",
      "source": [
        "# check class imbalance\n",
        "print(Counter(train_labels))\n",
        "\n",
        "# oversampling w/ replacement (minority labels)\n",
        "train_df=pd.DataFrame(\n",
        "    {'train_text': train_data,\n",
        "     'train_labels': train_labels})\n",
        "\n",
        "train_class0=train_df[train_df.train_labels==0]\n",
        "train_class1=train_df[train_df.train_labels==1]\n",
        "train_class2=train_df[train_df.train_labels==2]\n",
        "train_oversampled_class0=resample(train_class0, replace=True, n_samples=len(train_class2), random_state=12)\n",
        "train_oversampled_class1=resample(train_class1, replace=True, n_samples=len(train_class2), random_state=12)\n",
        "train_oversampled_df=pd.concat([train_oversampled_class0, train_oversampled_class1, train_class2])\n",
        "\n",
        "# get new train data, labels\n",
        "train_oversampled_data=train_oversampled_df[\"train_text\"].tolist()\n",
        "train_oversampled_labels=train_oversampled_df[\"train_labels\"].tolist()\n",
        "\n",
        "# check class balance\n",
        "print(Counter(train_oversampled_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({2: 419707, 0: 134640, 1: 89547})\n",
            "Counter({0: 419707, 1: 419707, 2: 419707})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0qfp10XZj-RA"
      },
      "cell_type": "markdown",
      "source": [
        "## Final model: Logistic Regression - Unigrams only\n",
        "\n",
        "using tf-idf, train Logistic Regression model to identify words with largest weights for each rating category"
      ]
    },
    {
      "metadata": {
        "id": "tJLnDSItj-RB",
        "outputId": "edc63740-2536-418b-865a-52e359d12c06"
      },
      "cell_type": "code",
      "source": [
        "# tf-idf\n",
        "#vec=TfidfVectorizer(preprocessor=preprocessor, ngram_range=(1,2), stop_words=total_stopwords, max_features=10000) #1-2grams\n",
        "vec=TfidfVectorizer(preprocessor=preprocessor, ngram_range=(1,1), stop_words=total_stopwords, max_features=10000) #unigram only\n",
        "vec_train_data=vec.fit_transform(train_oversampled_data) # training w/ oversampled data\n",
        "vec_test_data=vec.transform(test_data) \n",
        "\n",
        "# train Logistic Regression\n",
        "logit=LogisticRegression(penalty='l2')\n",
        "logit.fit(vec_train_data, train_oversampled_labels) # training w/ oversampled data\n",
        "pred_labels=logit.predict(vec_test_data)\n",
        "    \n",
        "# assess model\n",
        "f1=f1_score(test_labels, pred_labels, average=\"weighted\") \n",
        "accuracy=accuracy_score(test_labels, pred_labels)\n",
        "confusion=confusion_matrix(test_labels, pred_labels)\n",
        "print(\"logistic regression f1 score: %.3f\" %(f1))\n",
        "print(\"logistic regression accuracy score: %.3f\" %(accuracy))\n",
        "print(\"logistic regression confusion matrix:\")\n",
        "print(confusion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic regression f1 score: 0.811\n",
            "logistic regression accuracy score: 0.797\n",
            "logistic regression confusion matrix:\n",
            "[[27133  5494  1046]\n",
            " [ 4594 13509  4355]\n",
            " [ 2735 14499 87609]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8WlftHpaj-RF",
        "outputId": "c4cf78e2-5943-4e3c-b0f6-2d489990acba"
      },
      "cell_type": "code",
      "source": [
        "# examine features w/ greatest weights\n",
        "\n",
        "def top_features(coefs, num_feats):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "    coefs: array of shape (num_labels, vocab_size)\n",
        "    num_feats: number of top features\n",
        "    \n",
        "    Prints:\n",
        "    top num_feats features with great weights by\n",
        "    rating category \n",
        "    \"\"\"   \n",
        "    \n",
        "    feats=[]\n",
        "    \n",
        "    # identify top coefs per rating category\n",
        "    top_indices=np.argsort(coefs, axis=1)[:,-num_feats:] # (num_labels, num_feats) \n",
        "    \n",
        "    # display feature, weight\n",
        "    for r in range(top_indices.shape[0]):\n",
        "        label_feats=[]\n",
        "        print(\"rating category %d\" %(r))\n",
        "        for c in range(top_indices.shape[1]):\n",
        "            feat=vec.get_feature_names()[top_indices[r,c]]\n",
        "            label_feats.append(feat)\n",
        "            weight=round(coefs[r, top_indices[r,c]], 2)\n",
        "            print(feat, weight)\n",
        "        feats.append(label_feats)\n",
        "        print(\"\")\n",
        "        \n",
        "    return feats\n",
        "\n",
        "feats=top_features(logit.coef_, num_feats=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rating category 0\n",
            "nie 5.01\n",
            "poor 5.09\n",
            "bland 5.11\n",
            "flavorless 5.14\n",
            "kalt 5.16\n",
            "inedible 5.19\n",
            "disappointing 5.19\n",
            "sick 5.26\n",
            "unacceptable 5.31\n",
            "disappointment 5.57\n",
            "cockroach 5.64\n",
            "tasteless 5.79\n",
            "zero 5.85\n",
            "awful 6.11\n",
            "disgusting 6.37\n",
            "waste 6.42\n",
            "terrible 6.44\n",
            "horrible 6.85\n",
            "poisoning 7.39\n",
            "worst 9.74\n",
            "\n",
            "rating category 1\n",
            "necessarily 3.46\n",
            "good 3.48\n",
            "average 3.71\n",
            "allerdings 3.75\n",
            "insgesamt 3.76\n",
            "serviceable 3.8\n",
            "pinch 3.95\n",
            "alright 4.34\n",
            "foodwise 4.46\n",
            "okay 4.53\n",
            "torn 4.55\n",
            "stars 4.82\n",
            "ok 4.93\n",
            "feelings 5.21\n",
            "decent 5.34\n",
            "conflicted 5.41\n",
            "aokay 6.09\n",
            "3star 6.97\n",
            "3stars 8.68\n",
            "aok 14.7\n",
            "\n",
            "rating category 2\n",
            "heaven 5.29\n",
            "hooked 5.54\n",
            "heavenly 5.54\n",
            "love 5.62\n",
            "disappoint 5.65\n",
            "wonderful 5.69\n",
            "gem 5.79\n",
            "highly 5.8\n",
            "incredible 5.9\n",
            "perfectly 5.94\n",
            "fantastic 6.33\n",
            "best 6.4\n",
            "perfect 6.61\n",
            "awesome 6.71\n",
            "perfection 6.9\n",
            "excellent 7.0\n",
            "great 7.0\n",
            "exceeded 7.71\n",
            "amazing 8.11\n",
            "delicious 8.99\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i1uCp70Mj-RJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Explore top words\n",
        "\n",
        "Use GloVe word embeddings to explore top features"
      ]
    },
    {
      "metadata": {
        "id": "ye4RGkAn7rq-"
      },
      "cell_type": "code",
      "source": [
        "# Install a few python packages using pip\n",
        "from common import utils\n",
        "utils.require_package(\"wget\")      # for fetching dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fHLx_Cmuj-RK",
        "outputId": "aa5072ce-95a7-4796-8961-8d6edef2d776"
      },
      "cell_type": "code",
      "source": [
        "# get word embeddings\n",
        "hands=glove_helper.Hands(ndim=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading vectors from data/glove/glove.6B.zip\n",
            "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
            "Found 400,000 words.\n",
            "Parsing vectors... Done! (W.shape = (400003, 100))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AvDUjgsGj-RN"
      },
      "cell_type": "code",
      "source": [
        "# compute cosine similarity\n",
        "\n",
        "def find_nn_cos(v, Wv, k=10):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      v: (d-dimensional vector) word vector of interest\n",
        "      Wv: (V x d matrix) word embeddings\n",
        "      k: (int) number of neighbors to return\n",
        "    \n",
        "    Returns (nns, ds), where:\n",
        "      nns: (k-dimensional vector of int), row indices of nearest neighbors, \n",
        "      which may include the given word\n",
        "      similarities: (k-dimensional vector of float), cosine similarity of each \n",
        "      neighbor in nns\n",
        "      \n",
        "    \"\"\"\n",
        "    \n",
        "    # compute cosine similarity\n",
        "    cosin_sim = np.array([np.dot(v.T, Wv[r]) / (np.linalg.norm(v) * np.linalg.norm(Wv[r])) for r in range(Wv.shape[0])])\n",
        "                         \n",
        "    nns = np.argsort(-cosin_sim)[:k]\n",
        "    similarities = cosin_sim[nns]\n",
        "                         \n",
        "    return (nns, similarities)\n",
        "\n",
        "\n",
        "# identify nearest neighbors\n",
        "\n",
        "def show_nns(hands, word, k=5):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    hands: word embeddings\n",
        "    word: string\n",
        "    k: number of nearest neighbors\n",
        "    \n",
        "    Prints:\n",
        "    k nearest neighbors of specified word\n",
        "    \n",
        "    \"\"\"\n",
        "    word=word.lower()\n",
        "    print(\"nearest neighbors for '{:s}'\".format(word))\n",
        "    v=hands.get_vector(word)\n",
        "    for i, sim in zip(*find_nn_cos(v, hands.W, k)):\n",
        "        target_word=hands.vocab.id_to_word[i]\n",
        "        print(\"{:.03f} : '{:s}'\".format(sim, target_word))\n",
        "    print(\"\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gaSLk-EIj-RR"
      },
      "cell_type": "code",
      "source": [
        "# identify nearest neighbors for every word in feats\n",
        "for label in feats:\n",
        "    for word in label:\n",
        "        show_nns(hands, word)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "otVmjzYY7rro"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparisons using GloVe"
      ]
    },
    {
      "metadata": {
        "id": "JTAY2_mU7rrq"
      },
      "cell_type": "code",
      "source": [
        "## Update these variables to the list(s) of words you're interested in for the following sections\n",
        "words_of_interest = feats[1][10:] # words you want to compare\n",
        "compare = feats[2] # words you want to compare them to\n",
        "compare\n",
        "# compare = ['torn', 'stars', 'ok', 'feelings', 'decent', 'conflicted', 'okay', '3star', '3stars', 'aok']\n",
        "# compare = ['never', 'poor', 'bland', 'flavorless', 'cold', 'inedible', 'disappointing', 'sick',  'unacceptable', \n",
        "#            'disappointment', 'cockroach','tasteless', 'zero', 'awful', 'disgusting', 'waste', 'terrible', 'horrible',\n",
        "#            'poisoning', 'worst']\n",
        "\n",
        "# Pull vectors and IDs for the compare words from hands (no need to update)\n",
        "compare_vectors = np.array([hands.get_vector(word) for word in compare])\n",
        "compare_ids =hands.vocab.words_to_ids(compare)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7KbusuB7rrw"
      },
      "cell_type": "code",
      "source": [
        "# find nns given a comparison list\n",
        "\n",
        "def show_nns_compare_list(hands_words, hands_vectors, word, k=5):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    hands_words: list of the words being investicated (strings)\n",
        "    hands_vectors: word embeddings (vectors only) of a specified list\n",
        "    word: string\n",
        "    k: number of nearest neighbors\n",
        "    \n",
        "    Prints:\n",
        "    k nearest neighbors of specified word in a specified list\n",
        "    \n",
        "    \"\"\"\n",
        "    word=word.lower()\n",
        "    print(\"nearest neighbors for '{:s}'\".format(word))\n",
        "    v=hands.get_vector(word)\n",
        "    for i, sim in zip(*find_nn_cos(v, hands_vectors, k)):\n",
        "        target_word=hands_words[i]\n",
        "        print(\"{:.03f} : '{:s}'\".format(sim, target_word))\n",
        "    print(\"\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A44-NpUd7rr6",
        "outputId": "3c5a2a31-3cdf-45b3-b6f1-b421740fc31d"
      },
      "cell_type": "code",
      "source": [
        "## Compare a list of words to another list of words - find nearest neighbors\n",
        "for word in words_of_interest:\n",
        "    show_nns_compare_list(compare, compare_vectors, word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nearest neighbors for 'nie'\n",
            "1.000 : 'nie'\n",
            "0.199 : 'kalt'\n",
            "0.147 : 'cockroach'\n",
            "0.135 : 'flavorless'\n",
            "0.083 : 'inedible'\n",
            "\n",
            "nearest neighbors for 'poor'\n",
            "1.000 : 'poor'\n",
            "0.569 : 'worst'\n",
            "0.569 : 'sick'\n",
            "0.498 : 'disappointing'\n",
            "0.457 : 'terrible'\n",
            "\n",
            "nearest neighbors for 'bland'\n",
            "1.000 : 'bland'\n",
            "0.457 : 'tasteless'\n",
            "0.415 : 'disgusting'\n",
            "0.402 : 'awful'\n",
            "0.307 : 'poor'\n",
            "\n",
            "nearest neighbors for 'flavorless'\n",
            "1.000 : 'flavorless'\n",
            "0.529 : 'inedible'\n",
            "0.461 : 'tasteless'\n",
            "0.302 : 'bland'\n",
            "0.198 : 'disgusting'\n",
            "\n",
            "nearest neighbors for 'kalt'\n",
            "1.000 : 'kalt'\n",
            "0.199 : 'nie'\n",
            "0.147 : 'flavorless'\n",
            "0.107 : 'inedible'\n",
            "0.082 : 'tasteless'\n",
            "\n",
            "nearest neighbors for 'inedible'\n",
            "1.000 : 'inedible'\n",
            "0.529 : 'flavorless'\n",
            "0.480 : 'tasteless'\n",
            "0.316 : 'disgusting'\n",
            "0.301 : 'bland'\n",
            "\n",
            "nearest neighbors for 'disappointing'\n",
            "1.000 : 'disappointing'\n",
            "0.621 : 'disappointment'\n",
            "0.568 : 'worst'\n",
            "0.517 : 'awful'\n",
            "0.498 : 'poor'\n",
            "\n",
            "nearest neighbors for 'sick'\n",
            "1.000 : 'sick'\n",
            "0.569 : 'poor'\n",
            "0.514 : 'terrible'\n",
            "0.494 : 'awful'\n",
            "0.490 : 'horrible'\n",
            "\n",
            "nearest neighbors for 'unacceptable'\n",
            "1.000 : 'unacceptable'\n",
            "0.518 : 'disgusting'\n",
            "0.493 : 'terrible'\n",
            "0.430 : 'horrible'\n",
            "0.385 : 'tasteless'\n",
            "\n",
            "nearest neighbors for 'disappointment'\n",
            "1.000 : 'disappointment'\n",
            "0.621 : 'disappointing'\n",
            "0.537 : 'terrible'\n",
            "0.490 : 'awful'\n",
            "0.455 : 'worst'\n",
            "\n",
            "nearest neighbors for 'cockroach'\n",
            "1.000 : 'cockroach'\n",
            "0.266 : 'inedible'\n",
            "0.174 : 'flavorless'\n",
            "0.159 : 'disgusting'\n",
            "0.147 : 'nie'\n",
            "\n",
            "nearest neighbors for 'tasteless'\n",
            "1.000 : 'tasteless'\n",
            "0.693 : 'disgusting'\n",
            "0.480 : 'inedible'\n",
            "0.461 : 'flavorless'\n",
            "0.457 : 'bland'\n",
            "\n",
            "nearest neighbors for 'zero'\n",
            "1.000 : 'zero'\n",
            "0.449 : 'worst'\n",
            "0.345 : 'poor'\n",
            "0.342 : 'unacceptable'\n",
            "0.336 : 'waste'\n",
            "\n",
            "nearest neighbors for 'awful'\n",
            "1.000 : 'awful'\n",
            "0.922 : 'horrible'\n",
            "0.874 : 'terrible'\n",
            "0.702 : 'disgusting'\n",
            "0.582 : 'worst'\n",
            "\n",
            "nearest neighbors for 'disgusting'\n",
            "1.000 : 'disgusting'\n",
            "0.737 : 'horrible'\n",
            "0.702 : 'awful'\n",
            "0.693 : 'tasteless'\n",
            "0.605 : 'terrible'\n",
            "\n",
            "nearest neighbors for 'waste'\n",
            "1.000 : 'waste'\n",
            "0.410 : 'poisoning'\n",
            "0.373 : 'sick'\n",
            "0.366 : 'poor'\n",
            "0.342 : 'unacceptable'\n",
            "\n",
            "nearest neighbors for 'terrible'\n",
            "1.000 : 'terrible'\n",
            "0.919 : 'horrible'\n",
            "0.874 : 'awful'\n",
            "0.675 : 'worst'\n",
            "0.605 : 'disgusting'\n",
            "\n",
            "nearest neighbors for 'horrible'\n",
            "1.000 : 'horrible'\n",
            "0.922 : 'awful'\n",
            "0.919 : 'terrible'\n",
            "0.737 : 'disgusting'\n",
            "0.608 : 'worst'\n",
            "\n",
            "nearest neighbors for 'poisoning'\n",
            "1.000 : 'poisoning'\n",
            "0.422 : 'sick'\n",
            "0.410 : 'waste'\n",
            "0.371 : 'worst'\n",
            "0.315 : 'terrible'\n",
            "\n",
            "nearest neighbors for 'worst'\n",
            "1.000 : 'worst'\n",
            "0.675 : 'terrible'\n",
            "0.608 : 'horrible'\n",
            "0.582 : 'awful'\n",
            "0.569 : 'poor'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WOKC5UGN7rsI"
      },
      "cell_type": "code",
      "source": [
        "def similar_dissimilar_words(hands_words, Wv):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      hands_words: list of the words being investicated (strings)\n",
        "      Wv: (V x d matrix) word embeddings\n",
        "    \n",
        "    Returns (most_similar_words, most_similar_score), (least_similar_words, least_similar_score)\n",
        "      \n",
        "    \"\"\"\n",
        "    k = 1\n",
        "    \n",
        "    # create each combination of 2 words\n",
        "    combos = [combo for combo in itertools.combinations(Wv, 2)]\n",
        "    combo_words = [combo for combo in itertools.combinations(hands_words, 2)]\n",
        "    #print(combos)\n",
        "    \n",
        "    #calculate cosine similarity for each combo of 2 words\n",
        "    cosin_sim = np.array([np.dot(combo[0].T, combo[1]) / (np.linalg.norm(combo[0]) * np.linalg.norm(combo[1])) \n",
        "                          for combo in combos])\n",
        "    \n",
        "    #identify most similar (highest cosine similarity) and dissimilar (lowest cosine similarity)\n",
        "    most_similar_idx = int(np.argsort(-cosin_sim)[:k])\n",
        "    most_similar_words = combo_words[most_similar_idx]\n",
        "    most_similar_score = cosin_sim[most_similar_idx]\n",
        "    \n",
        "    least_similar_idx = int(np.argsort(cosin_sim)[:k])\n",
        "    least_similar_words = combo_words[least_similar_idx]\n",
        "    least_similar_score = cosin_sim[least_similar_idx]\n",
        "    \n",
        "                         \n",
        "    return ((most_similar_words, most_similar_score), (least_similar_words, least_similar_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KQxFtN6t7rsU",
        "outputId": "68370772-51f8-4c4c-87f3-9c2cfd66fa84"
      },
      "cell_type": "code",
      "source": [
        "print(similar_dissimilar_words(compare, compare_vectors))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((('feelings', '3star'), 0.92197144), (('torn', 'conflicted'), -0.039125863))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UX6cOYnw7rsa"
      },
      "cell_type": "code",
      "source": [
        "def most_unique_words(hands_words, Wv):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      hands_words: list of the words being investicated (strings)\n",
        "      Wv: (V x d matrix) word embeddings\n",
        "\n",
        "    \n",
        "    Returns (sum_smallest, max_smallest), where:\n",
        "      sum_smallest: word where the sum of cosine similarity of all words in list is smallest \n",
        "      max_smallest: word where the max of all the cosine similarities in the list is smallest\n",
        "      \n",
        "    \"\"\"\n",
        "    k=1\n",
        "    # sum together all cosine similarities - return word with smallest sum\n",
        "    cosin_sim = dict()\n",
        "        \n",
        "    for vector, word in zip(Wv, hands_words):\n",
        "        cosin_sim[word] = sum([np.dot(vector.T, Wv[r]) / (np.linalg.norm(vector) * np.linalg.norm(Wv[r])) \n",
        "                          for r in range(Wv.shape[0])]) \n",
        "      \n",
        "    sum_smallest = min(cosin_sim, key=lambda k: cosin_sim[k])             \n",
        "    \n",
        "    # take max of all cosine similarities - return word with smallest max \n",
        "    cosin_sim_maxes = dict()\n",
        "            \n",
        "    for vector, word in zip(Wv, hands_words):\n",
        "        cosin_sim_maxes[word] = max([np.dot(vector.T, Wv[r]) / (np.linalg.norm(vector) * np.linalg.norm(Wv[r])) \n",
        "                          for r in range(Wv.shape[0]) if word != hands_words[r]]) \n",
        "      \n",
        "    max_smallest = min(cosin_sim_maxes, key=lambda k: cosin_sim_maxes[k])  \n",
        "        \n",
        "                         \n",
        "    return (sum_smallest, max_smallest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jRLHMF4y7rse",
        "outputId": "bf834402-6884-41be-de26-21823555fc99"
      },
      "cell_type": "code",
      "source": [
        "print(most_unique_words(compare,compare_vectors))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('exceeded', 'disappoint')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PzDBA2I47rsi"
      },
      "cell_type": "markdown",
      "source": [
        "## K-means clustering"
      ]
    },
    {
      "metadata": {
        "id": "_EeFIaj07rsk"
      },
      "cell_type": "code",
      "source": [
        "# create matrix\n",
        "def create_mat(words, embeds):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    words: list of words to cluster\n",
        "    embeds: matrix of word embeddings\n",
        "    \n",
        "    Returns:\n",
        "    mat: (words_len, embed_dim) matrix\n",
        "    of word embeddings\n",
        "    \"\"\"\n",
        "    mat=np.zeros((len(words), embeds.shape[1])) # (20,100)\n",
        "\n",
        "    for r in range(mat.shape[0]):\n",
        "        mat[r,]=hands.get_vector(words[r])\n",
        "        \n",
        "    return mat\n",
        "\n",
        "\n",
        "def show_clusters(words, clusters):\n",
        "    all_clusters=[[] for num in range(len(set(clusters)))]\n",
        "    \n",
        "    for i, c in enumerate(clusters):\n",
        "        all_clusters[c].append(words[i])\n",
        "        \n",
        "    for j, c in enumerate(all_clusters):\n",
        "        print(\"cluster %d\" %(j+1))\n",
        "        for w in all_clusters[j]:\n",
        "            print(w)\n",
        "        print(\"\")\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WZTFNdtc7rso"
      },
      "cell_type": "code",
      "source": [
        "#redefining compare to test different things       \n",
        "#compare = [word for n in range(len(feats)) for word in feats[n]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Psry1MN7rsq"
      },
      "cell_type": "code",
      "source": [
        "# choose # of clusters, create embeddings\n",
        "num_clusters=4\n",
        "word_embeds=create_mat(compare, hands)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9BVeFNA7rsu",
        "outputId": "7c828c48-1cf8-4852-a276-7bdf52691e0e"
      },
      "cell_type": "code",
      "source": [
        "# k-means clustering with nltk\n",
        "\n",
        "\n",
        "kclusterer=KMeansClusterer(num_clusters, distance=cosine_distance, repeats=300)\n",
        "assigned_clusters=kclusterer.cluster(word_embeds, assign_clusters=True)\n",
        "\n",
        "show_clusters(compare, assigned_clusters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster 1\n",
            "disappointing\n",
            "unacceptable\n",
            "disappointment\n",
            "terrible\n",
            "\n",
            "cluster 2\n",
            "poor\n",
            "bland\n",
            "zero\n",
            "worst\n",
            "\n",
            "cluster 3\n",
            "never\n",
            "cold\n",
            "sick\n",
            "awful\n",
            "disgusting\n",
            "waste\n",
            "horrible\n",
            "poisoning\n",
            "\n",
            "cluster 4\n",
            "flavorless\n",
            "inedible\n",
            "cockroach\n",
            "tasteless\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8d22fAZU7rs0",
        "outputId": "ddf33fb4-596d-4c18-9f0b-42c8fb42b066"
      },
      "cell_type": "code",
      "source": [
        "# k-means clustering with sklearn\n",
        "\n",
        "skmeans_model = KMeans(init='k-means++',n_clusters=num_clusters, n_init=5000)\n",
        "skmeans_model.fit(word_embeds)\n",
        "\n",
        "cluster_labels = skmeans_model.labels_\n",
        "\n",
        "show_clusters(compare, cluster_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster 1\n",
            "cold\n",
            "sick\n",
            "zero\n",
            "waste\n",
            "poisoning\n",
            "\n",
            "cluster 2\n",
            "unacceptable\n",
            "awful\n",
            "disgusting\n",
            "terrible\n",
            "horrible\n",
            "\n",
            "cluster 3\n",
            "never\n",
            "poor\n",
            "disappointing\n",
            "disappointment\n",
            "worst\n",
            "\n",
            "cluster 4\n",
            "bland\n",
            "flavorless\n",
            "inedible\n",
            "cockroach\n",
            "tasteless\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TAxqx77u_KsY"
      },
      "cell_type": "markdown",
      "source": [
        "# POS Tagging"
      ]
    },
    {
      "metadata": {
        "id": "8qJ-eKCg-NGK"
      },
      "cell_type": "code",
      "source": [
        "#create dictionary of POS tags in corpus, using nltk's pos_tag \n",
        "\n",
        "tag_dict = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "for review in text:\n",
        "    for tup in pos_tag(review.split()):\n",
        "        input_token = tokenize_text(tup[0])\n",
        "        if input_token:\n",
        "            can_token = canonicalize_tokens(input_token)[0]\n",
        "            tag_dict[can_token][tup[1]] += 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JNJstIdq_gvW"
      },
      "cell_type": "code",
      "source": [
        "def likely_POS(word):\n",
        "  '''Return count-based likely POS from tag_dict\n",
        "  If word missing from corpus, use pos_tag to tag (out of context)'''\n",
        "  \n",
        "    if word in tag_dict.keys():\n",
        "        return max(tag_dict[word], key=tag_dict[word].get)\n",
        "    else:\n",
        "        return pos_tag([word])[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuihGg3q_7sY"
      },
      "cell_type": "code",
      "source": [
        "#Return top features that are just a specified part of speech\n",
        "\n",
        "def top_noun_features(coefs, num_feats):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "    coefs: array of shape (num_labels, vocab_size)\n",
        "    num_feats: number of top features\n",
        "    \n",
        "    Prints:\n",
        "    top num_feats features with great weights by\n",
        "    rating category \n",
        "    \"\"\"   \n",
        "    \n",
        "    feats=[]\n",
        "    \n",
        "    # identify top coefs per rating category\n",
        "    top_indices=np.argsort(coefs, axis=1)[:,-num_feats:] # (num_labels, num_feats) \n",
        "    \n",
        "    # display feature, weight\n",
        "    for r in range(top_indices.shape[0]):\n",
        "        label_feats=[]\n",
        "        print(\"rating category %d\" %(r))\n",
        "        for c in range(top_indices.shape[1]):\n",
        "            feat=vec.get_feature_names()[top_indices[r,c]]\n",
        "            if likely_POS(feat)[0:2] == \"NN\": #nouns\n",
        "#             if likely_POS(feat)[0:2] == \"VB\": #verbs\n",
        "#             if likely_POS(feat)[0:2] in [\"JJ\", \"RB\"]: #adjectives and adverbs\n",
        "                label_feats.append(feat)\n",
        "                weight=round(coefs[r, top_indices[r,c]], 2)\n",
        "                print(likely_POS(feat), feat, weight)\n",
        "        feats.append(label_feats)\n",
        "        print(\"\")\n",
        "        \n",
        "    return feats\n",
        "\n",
        "noun_feats=top_noun_features(logit.coef_, num_feats=70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0nvoTpwhASoa"
      },
      "cell_type": "code",
      "source": [
        "#POS counts using just nltk \n",
        "\n",
        "tagged_feats = list()\n",
        "\n",
        "for i, feat in enumerate(feats):\n",
        "    tagged_feats.append(pos_tag(feat))\n",
        "    \n",
        "tagged_feats\n",
        "\n",
        "pos_list = [tagged_feats[i][n][1] for i in range(len(tagged_feats)) for n in range(len(tagged_feats[i]))]\n",
        "pos_set = set(pos_list)\n",
        "\n",
        "cnt = Counter()\n",
        "for word in pos_list:\n",
        "     cnt[word] += 1\n",
        "\n",
        "cnt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jm39kg5AATzC"
      },
      "cell_type": "code",
      "source": [
        "# POS counter using revised tag_dict\n",
        "\n",
        "tagged_feats2 = list()\n",
        "flat_feats = list()\n",
        "\n",
        "\n",
        "for i in range(len(feats)):\n",
        "    for feat in feats[i]:\n",
        "        flat_feats.append(feat)\n",
        "\n",
        "count2 = Counter()\n",
        "for feat in flat_feats:\n",
        "#     print(feat)\n",
        "    count2[likely_POS(feat)] += 1\n",
        "    \n",
        "count2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}